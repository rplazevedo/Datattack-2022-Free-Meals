{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kvdaXMnkdJ4O"},"outputs":[],"source":["import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import numpy as np\n","import random\n","import pandas as pd\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import convert_image_dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32610,"status":"ok","timestamp":1652570524767,"user":{"displayName":"Pedro Serrano","userId":"03220631262079591425"},"user_tz":-60},"id":"_i-RNa2DA5m6","outputId":"73071657-696b-452d-c7d8-7be46e24d8ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucOKu_TtfCLM"},"outputs":[],"source":["class myOwnDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0][\"file_name\"]\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, path))\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i][\"bbox\"][0]\n","            ymin = coco_annotation[i][\"bbox\"][1]\n","            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n","            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # Labels (In my case, I only one class: target class or background)\n","        # labels = torch.ones((num_objs,), dtype=torch.int64)\n","        labels = []\n","        for i in range(num_objs):\n","            label = coco_annotation[i][\"category_id\"]\n","            labels.append(label)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i][\"area\"])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","\n","# In my case, just added ToTensor\n","def get_transform():\n","    custom_transforms = []\n","    custom_transforms.append(torchvision.transforms.ToTensor())\n","    return torchvision.transforms.Compose(custom_transforms)\n","\n","\n","# collate_fn needs for batch\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model"]},{"cell_type":"code","source":["def printDistances(distances, token1Length, token2Length):\n","    for t1 in range(token1Length + 1):\n","        for t2 in range(token2Length + 1):\n","            print(int(distances[t1][t2]), end=\" \")\n","        print()\n","\n","def levenshteinDistanceDP(token1, token2):\n","    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n","\n","    for t1 in range(len(token1) + 1):\n","        distances[t1][0] = t1\n","\n","    for t2 in range(len(token2) + 1):\n","        distances[0][t2] = t2\n","        \n","    a = 0\n","    b = 0\n","    c = 0\n","    \n","    for t1 in range(1, len(token1) + 1):\n","        for t2 in range(1, len(token2) + 1):\n","            if (token1[t1-1] == token2[t2-1]):\n","                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n","            else:\n","                a = distances[t1][t2 - 1]\n","                b = distances[t1 - 1][t2]\n","                c = distances[t1 - 1][t2 - 1]\n","                \n","                if (a <= b and a <= c):\n","                    distances[t1][t2] = a + 1\n","                elif (b <= a and b <= c):\n","                    distances[t1][t2] = b + 1\n","                else:\n","                    distances[t1][t2] = c + 1\n","\n","    #printDistances(distances, len(token1), len(token2))\n","    return distances[len(token1)][len(token2)]\n"],"metadata":{"id":"40dCN0gVlXAA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def out_to_str(result, thresh=0.7):\n","    \"\"\"\n","    Sort and order the predicted digits into one or two serial numbers.\n","    'result' is the dictionary containing the prediction dictionary with keys\n","             'boxes', 'labels' and 'scores'\n","    'thres' is the score value for rejecting false detections\n","    \"\"\"\n","\n","    def split(arr, cond):\n","      return [arr[cond], arr[~cond]]\n","    if 'scores' in result.keys():\n","        indices = result['scores'] > thresh\n","        boxes = np.array((result['boxes'][indices]).tolist())\n","        labels = np.array((result['labels'][indices]).tolist())-1\n","    else:\n","        boxes = np.array((result['boxes']).tolist())\n","        labels = np.array((result['labels']).tolist())-1\n","\n","    if labels.shape[0] == 0:\n","      return \"\",\"\"\n","    if len(boxes.shape) == 1:\n","      return str(labels[0]), \"\"\n","\n","    y_sort = boxes[:, 1].argsort()\n","    box_sorted_asc = boxes[y_sort]\n","    labels_sorted_asc = labels[y_sort]\n","    avg_y = np.array([0.5*(box[1]+box[3]) for box in box_sorted_asc])\n","\n","    is_first_row = avg_y<box_sorted_asc[0,3]\n","    fst_row, sec_row = split(labels_sorted_asc, is_first_row)\n","    fst_box, sec_box = split(box_sorted_asc, is_first_row)\n","\n","    x_sort_first = fst_box[:, 0].argsort()\n","    first_label = fst_row[x_sort_first]\n","    row_1_str = \"\".join(map(str,first_label))\n","\n","    x_sort_sec = sec_box[:, 0].argsort()\n","    sec_label = sec_row[x_sort_sec]\n","    row_2_str = \"\".join(map(str,sec_label))\n","    \n","    return row_1_str, row_2_str"],"metadata":{"id":"0VePxmPAK1a4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def export_predictions(images_dir: str, team_name: str, *args) -> None:\n","    \"\"\"\n","    Go through all images, get the predicted strings and export them to .csv\n","\n","    Args:\n","        images_dir (str): the path to the images' folder\n","        team_name (str): the name of the team to include in the exported .csv file\n","    \"\"\"\n","    \n","    predictions = []\n","    for image in os.listdir(images_dir):\n","        #print(images_dir+'/'+image)\n","        img_read = read_image(images_dir+'/'+image)\n","        img_read = img_read.to(device)\n","        batch = torch.stack([img_read])\n","        batch = convert_image_dtype(batch, dtype=torch.float)\n","        \n","        outputs = model(batch)\n","        # get predictions\n","        predicted_string_1, predicted_string_2 = out_to_str(outputs[0])\n","        \n","        # store predictions\n","        predictions.append({\"image_name\": image, \n","                            \"string_1_prediction\": predicted_string_1, \n","                            \"string_2_prediction\": predicted_string_2})\n","\n","    # export predictions\n","    results_df = pd.DataFrame(predictions)\n","    results_df.to_csv(f\"submissions/{team_name}.csv\", index=False, header=True)\n"],"metadata":{"id":"W_VWecGdzDEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjgSQEg1fFd6"},"outputs":[],"source":["# path to your own data and coco file\n","train_data_dir = \"/content/drive/Shareddrives/Datattack (Free Meals)/train/data\"\n","train_coco = \"/content/drive/Shareddrives/Datattack (Free Meals)/train/data/labels_train.json\"\n","\n","# Batch size\n","train_batch_size = 1\n","\n","# Params for dataloader\n","train_shuffle_dl = True\n","num_workers_dl = 2\n","\n","# Params for training\n","\n","# Two classes; Only target class or background\n","num_classes = 11\n","num_epochs = 20\n","\n","lr = 0.0001\n","momentum = 0.9\n","weight_decay = 0.005"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1652542918886,"user":{"displayName":"Pedro Serrano","userId":"03220631262079591425"},"user_tz":-60},"id":"Fr9s9bKrrQxi","outputId":"de9417a3-fb26-4c76-ed2f-6b66a75bc1b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 1.11.0+cu113\n","loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n"]}],"source":["print(\"Torch version:\", torch.__version__)\n","\n","# create own Dataset\n","my_dataset = myOwnDataset(\n","    root=train_data_dir, annotation=train_coco, transforms=get_transform()\n",")\n","\n","# split the data\n","val_perc = 0.2\n","dataset_size = len(my_dataset)\n","val_size = int(val_perc * dataset_size)\n","train_size = dataset_size - val_size\n","train_data, val_data = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n","\n","# own DataLoader\n","train_loader = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=train_batch_size,\n","    shuffle=train_shuffle_dl,\n","    num_workers=num_workers_dl,\n","    collate_fn=collate_fn,\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_data,\n","    batch_size=train_batch_size,\n","    shuffle=train_shuffle_dl,\n","    num_workers=num_workers_dl,\n","    collate_fn=collate_fn,\n",")\n","\n","all_data_loader = torch.utils.data.DataLoader(\n","    my_dataset,\n","    batch_size=train_batch_size,\n","    shuffle=train_shuffle_dl,\n","    num_workers=num_workers_dl,\n","    collate_fn=collate_fn,\n",")\n","\n","\n","# select device (whether GPU or CPU)\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1gIIKMAsBLC"},"outputs":[],"source":["# DataLoader is iterable over Dataset\n","for imgs, annotations in train_loader:\n","    imgs = list(img.to(device) for img in imgs)\n","    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","    #print(annotations)\n","\n","for imgs, annotations in val_loader:\n","    imgs = list(img.to(device) for img in imgs)\n","    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","    #print(annotations)\n","\n","\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# parameters\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(\n","    params, lr=lr, momentum=momentum, weight_decay=weight_decay\n",")\n","\n","len_trainloader = len(train_loader)\n","len_valloader = len(val_loader)\n","\n"]},{"cell_type":"code","source":["model_name=\"resNet50_FPN_full\"\n","best_dist = 1000\n","print(device)\n","# Training\n","for epoch in range(num_epochs):\n","    print(f\"Epoch: {epoch}/{num_epochs}\")\n","\n","    print(\"Training Phase\")\n","\n","    model.train()\n","    \n","    for imgs, annotations in all_data_loader:\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        loss_dict = model(imgs, annotations)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","    \n","    print(\"Validation Phase\")\n","\n","    distance = 0\n","    dist_list = []\n","\n","    model.eval()\n","    preds = list()\n","    # Deactivate gradients\n","    with torch.no_grad():\n","\n","      i=0\n","      for imgs, annotations in val_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        preds = model(imgs)\n","\n","        for pred in preds:\n","          row1_str, row2_str = out_to_str(pred)\n","        for annotation in annotations:\n","          gt_row1, gt_row2 = out_to_str(annotation)\n","        \n","        distance1 = levenshteinDistanceDP(row1_str,gt_row1)\n","        distance2 = levenshteinDistanceDP(row2_str,gt_row2)\n","        t_distance = distance1 + distance2\n","\n","        dist_list.append(t_distance)\n","\n","\n","      run_dist = np.array(dist_list).mean()\n","      print(f\"Iteration: {i}/{len_valloader}, Loss: {losses}, Distance Avg = {run_dist}\") \n","    # Save checkpoint\n","    if np.array(dist_list).mean()<best_dist:\n","      best_dist = run_dist\n","      model_path = os.path.join(weights_dir, f\"{model_name}_continental.pt\")\n","      torch.save(model.state_dict(), model_path)\n","      print(f\"Successfully saved at: {model_path}\")"],"metadata":{"id":"zcIkUCadbhx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Results and Weights\n","    weights_dir = os.path.join(\"/content/drive/Shareddrives/Datattack (Free Meals)/train/\",\"trained_models\", \"weights\")\n","    if not os.path.isdir(weights_dir):\n","        os.makedirs(weights_dir)\n","    print(weights_dir)\n","    model_name=\"resNet50_FPN\"\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    model = get_model_instance_segmentation(num_classes)\n","    model_path = os.path.join(weights_dir, f\"{model_name}_continental.pt\")\n","    model.load_state_dict(torch.load(model_path, map_location = device))\n","    model.eval()\n","\n","    TEAM_NAME = \"FreeMeals\"\n","    TEST_DIR = \"dataset/test\"\n","\n","    export_predictions(TEAM_NAME, TEST_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"jvwVKM1kDh4_","executionInfo":{"status":"error","timestamp":1652573486526,"user_tz":-60,"elapsed":970,"user":{"displayName":"R. Azevedo","userId":"06233968212056808534"}},"outputId":"f255e9f6-e193-44c9-ae3d-c5914aca522a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-93765677b855>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def main():\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"id":"k7tQWkkbDANG"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"final_thing.ipynb","provenance":[{"file_id":"1-06zZWl3j6kq9ba5fg3Z3KgYyrRI7g1K","timestamp":1652571486817},{"file_id":"1HF-peyyv2c6bvVY_ybgA32ylzn8Mq_Jj","timestamp":1652539059384},{"file_id":"1K5nE49fNhZ7nf9cACPgUTJT6TQN44Phh","timestamp":1652536626750}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}